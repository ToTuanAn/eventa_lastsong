{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1173893c4f0ea56",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Chunked Pooling\n",
    "This notebooks explains how the chunked pooling can be implemented. First you need to install the requirements: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d02a920f-cde0-4035-9834-49b087aab5cc",
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a8fbc1e477db48",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Then we load a model which we want to use for the embedding. We choose `jinaai/jina-embeddings-v2-base-en` but any other model which supports mean pooling is possible. However, models with a large maximum context-length are preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1380abf7acde9517",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/eventa/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from chunked_pooling import chunked_pooling, chunk_by_sentences, chunk_by_config\n",
    "\n",
    "# load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('jinaai/jina-embeddings-v2-base-en', trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained('jinaai/jina-embeddings-v2-base-en', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc0c1162797ffb0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Now we define the text which we want to encode and split it into chunks. The `chunk_by_sentences` function also returns the span annotations. Those specify the number of tokens per chunk which is needed for the chunked pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ef392f3437ef82e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) tensor(0) \n",
      "tensor(0) tensor(6) Berlin\n",
      "tensor(7) tensor(9) is\n",
      "tensor(10) tensor(13) the\n",
      "tensor(14) tensor(21) capital\n",
      "tensor(22) tensor(25) and\n",
      "tensor(26) tensor(33) largest\n",
      "tensor(34) tensor(38) city\n",
      "tensor(39) tensor(41) of\n",
      "tensor(42) tensor(49) Germany\n",
      "tensor(49) tensor(50) ,\n",
      "tensor(51) tensor(55) both\n",
      "tensor(56) tensor(58) by\n",
      "tensor(59) tensor(63) area\n",
      "tensor(64) tensor(67) and\n",
      "tensor(68) tensor(70) by\n",
      "tensor(71) tensor(81) population\n",
      "tensor(81) tensor(82) .\n",
      "tensor(83) tensor(86) Its\n",
      "tensor(87) tensor(91) more\n",
      "tensor(92) tensor(96) than\n",
      "tensor(97) tensor(98) 3\n",
      "tensor(98) tensor(99) .\n",
      "tensor(99) tensor(101) 85\n",
      "tensor(102) tensor(109) million\n",
      "tensor(110) tensor(121) inhabitants\n",
      "tensor(122) tensor(126) make\n",
      "tensor(127) tensor(129) it\n",
      "tensor(130) tensor(133) the\n",
      "tensor(134) tensor(142) European\n",
      "tensor(143) tensor(148) Union\n",
      "tensor(148) tensor(149) '\n",
      "tensor(149) tensor(150) s\n",
      "tensor(151) tensor(155) most\n",
      "tensor(156) tensor(164) populous\n",
      "tensor(165) tensor(169) city\n",
      "tensor(169) tensor(170) ,\n",
      "tensor(171) tensor(173) as\n",
      "tensor(174) tensor(182) measured\n",
      "tensor(183) tensor(185) by\n",
      "tensor(186) tensor(196) population\n",
      "tensor(197) tensor(203) within\n",
      "tensor(204) tensor(208) city\n",
      "tensor(209) tensor(215) limits\n",
      "tensor(215) tensor(216) .\n",
      "tensor(217) tensor(220) The\n",
      "tensor(221) tensor(225) city\n",
      "tensor(226) tensor(228) is\n",
      "tensor(229) tensor(233) also\n",
      "tensor(234) tensor(237) one\n",
      "tensor(238) tensor(240) of\n",
      "tensor(241) tensor(244) the\n",
      "tensor(245) tensor(251) states\n",
      "tensor(252) tensor(254) of\n",
      "tensor(255) tensor(262) Germany\n",
      "tensor(262) tensor(263) ,\n",
      "tensor(264) tensor(267) and\n",
      "tensor(268) tensor(270) is\n",
      "tensor(271) tensor(274) the\n",
      "tensor(275) tensor(280) third\n",
      "tensor(281) tensor(289) smallest\n",
      "tensor(290) tensor(295) state\n",
      "tensor(296) tensor(298) in\n",
      "tensor(299) tensor(302) the\n",
      "tensor(303) tensor(310) country\n",
      "tensor(311) tensor(313) in\n",
      "tensor(314) tensor(319) terms\n",
      "tensor(320) tensor(322) of\n",
      "tensor(323) tensor(327) area\n",
      "tensor(327) tensor(328) .\n",
      "tensor(0) tensor(0) \n",
      "Chunks:\n",
      "- \"Berlin is the capital and largest city of Germany, both by area and by population.\"\n",
      "- \" Its more than 3.85 million inhabitants make it the European Union's most populous city, as measured by population within city limits.\"\n",
      "- \" The city is also one of the states of Germany, and is the third smallest state in the country in terms of area.\"\n",
      "[(1, 17), (17, 44), (44, 69)]\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Berlin is the capital and largest city of Germany, both by area and by population. Its more than 3.85 million inhabitants make it the European Union's most populous city, as measured by population within city limits. The city is also one of the states of Germany, and is the third smallest state in the country in terms of area.\"\n",
    "\n",
    "# determine chunks\n",
    "chunks, span_annotations = chunk_by_sentences(input_text, tokenizer)\n",
    "print('Chunks:\\n- \"' + '\"\\n- \"'.join(chunks) + '\"')\n",
    "print(span_annotations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac41fd1f0560da7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Now we encode the chunks with the traditional and the context-sensitive chunked pooling method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abe3d93b9e6609b9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# chunk before\n",
    "embeddings_traditional_chunking = model.encode(chunks)\n",
    "\n",
    "# chunk afterwards (context-sensitive chunked pooling)\n",
    "inputs = tokenizer(input_text, return_tensors='pt')\n",
    "model_output = model(**inputs)\n",
    "embeddings = chunked_pooling(model_output, [span_annotations])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84b1b9d48cb6367",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Finally, we compare the similarity of the word \"Berlin\" with the chunks. The similarity should be higher for the context-sensitive chunked pooling method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da0cec59a3ece76",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity_new(\"Berlin\", \"Berlin is the capital and largest city of Germany, both by area and by population.\"): 0.849546\n",
      "similarity_trad(\"Berlin\", \"Berlin is the capital and largest city of Germany, both by area and by population.\"): 0.8486218\n",
      "similarity_new(\"Berlin\", \" Its more than 3.85 million inhabitants make it the European Union's most populous city, as measured by population within city limits.\"): 0.8248903\n",
      "similarity_trad(\"Berlin\", \" Its more than 3.85 million inhabitants make it the European Union's most populous city, as measured by population within city limits.\"): 0.7084339\n",
      "similarity_new(\"Berlin\", \" The city is also one of the states of Germany, and is the third smallest state in the country in terms of area.\"): 0.8498009\n",
      "similarity_trad(\"Berlin\", \" The city is also one of the states of Germany, and is the third smallest state in the country in terms of area.\"): 0.7534553\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "cos_sim = lambda x, y: np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))\n",
    "\n",
    "berlin_embedding = model.encode('Berlin')\n",
    "\n",
    "for chunk, new_embedding, trad_embeddings in zip(chunks, embeddings, embeddings_traditional_chunking):\n",
    "    print(f'similarity_new(\"Berlin\", \"{chunk}\"):', cos_sim(berlin_embedding, new_embedding))\n",
    "    print(f'similarity_trad(\"Berlin\", \"{chunk}\"):', cos_sim(berlin_embedding, trad_embeddings))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eventa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
